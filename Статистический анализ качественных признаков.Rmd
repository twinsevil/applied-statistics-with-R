---
title: "Статистический анализ качественных признаков"
author: "Литвинцев И.С"
date: '26 декабря 2017 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Статистический анализ качественных признаков

Рассматриваются данные о влиянии смертности от несчастных случаев и других внешних воздействий (Y) на естественный прирост населения в 1997 году. Необходимо произвести анализ качественных признаков.


Естественный прирост в 1997 | Y $<$ 60 | Y $\ge$ 60 | сумма
--- | --- | --- | ---
низк. | 3 | 4 | 7
выс. | 10 | 4 | 14
сумма | 13 | 8 | 21


```{r tab}
tab <- c(3, 10, 4, 4)
tab <- matrix(tab, ncol = 2)
row.names(tab) <- c('-', '+')
colnames(tab) <- c('Y<60', 'Y>=60')
tab
```

## 1. Условная энтропия и количество информации

Условная энтропия вычисляется по формуле: 
$$H_{B_i}(\xi) = -\sum^N_{j = 1}p_j(B_i)log_2p_j(B_i),$$
где $p_j(B_i) = P(A_j|B_i)$ -- условные вероятности, а $B_i$ - события.

Напишем небольшую функцию, которая позволит вычислить нам условную энтропию.

```{r entropy}
Entropy <- function(x) -sum(x[x != 0] * log(x[x != 0], 2))

x <- tab
x[1,1] <- 0

-sum(x[x != 0] * log(x[x != 0], 2))
```

Теперь с помощью этой функции вычислим энтропию совместного распределения и энтропии 2-х случайных величин.

```{r entropy1}
H_x.y <- Entropy(tab / sum(tab))
H_x.y

H_x <- Entropy(rowSums(tab) / sum(tab))
H_x

H_y <- Entropy(colSums(tab) / sum(tab))
H_y
```

Теперь найдем количество информации, которое вычисляется по формуле: 
$$I(\xi, \eta) = H(\xi) + H(\eta) - H(\xi, \eta)$$

```{r I}
I <- H_x + H_y - H_x.y; I
```

Теперь мы можем вычислить односторонние коэффициенты неопределенности, которые измеряют долю информации одного признака, которую он разделяет с другим. Данные коэффициенты вычисляются по формулам: 
$$J_{X|Y} = \frac{I(X, Y)}{H(Y)}100\% \qquad J_{Y|X} = \frac{I(X, Y)}{H(X)}100\%.$$

```{r J}
J_x.y <- I / H_y * 100; J_x.y

J_y.x <- I / H_x * 100; J_y.x
```

А также вычислим симметричный коэффициент неопределенности, который вычисляется по формуле: $J = \frac{2I(X, Y)}{H(X) + H(Y)} * 100\%$.

```{r J1}
J <- 2 * I / (H_x + H_y) * 100; J
```


## 2. Точный критерий Фишера

Будем работать с прежней таблицей сопряженности.

```{r tab1}
tab
```

Точный критерий Фишера используется для проверки гипотезы однородности категориальных вероятностей в случае таблиц сопряженности размерности $2 \times 2$.

Чтобы вычислить доверительный уровень вероятности, который вычисляется по формуле 
$$\alpha_* = \sum_{i=0}^aP^i_{a+c}, \qquad P^a_{a+c} = \frac{C^a_{a+b}C^c_{c+d}}{C^{a+c}_n} = \frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!n!}$$
При заданных частотах $(a+b), \; (c+d), \; (a+c), \; (b+d)$ и $n$ можно перечислить все возможные таблицы с положительными элементами вида

| X / Y |   0   |       1       |         |
|:---:|:-----:|:-------------:|:-------:|
|  0  |   x   |     a+b-x     |   a+b   |
|  1  | a+c-x | (c+d)-(a+c-x) |   c+d   |
|     |  a+c  |      b+d      | a+b+c+d |


```{r tabs, include=FALSE}
tab2 <- c(0, 13, 7, 1)
tab2 <- matrix(tab2, ncol = 2)
row.names(tab2) <- c('Y<60', 'Y>=60')
colnames(tab2) <- c('-', '+')

tab3 <- c(1, 12, 6, 2)
tab3 <- matrix(tab3, ncol = 2)
row.names(tab3) <- c('Y<60', 'Y>=60')
colnames(tab3) <- c('-', '+')

tab4 <- c(2, 11, 5, 3)
tab4 <- matrix(tab4, ncol = 2)
row.names(tab4) <- c('Y<60', 'Y>=60')
colnames(tab4) <- c('-', '+')
```

```{r tabs1}
tab2

tab3

tab4
```

Теперь напишем небольшую функцию, которая для каждой таблицы вычислит $P^a_{a+c}$:

```{r P}
P <- function(tab) {
  a = tab[1,1]
  b = tab[1,2]
  c = tab[2,1]
  d = tab[2,2]
  p = (factorial(a+b)*factorial(c+d)*factorial(a+c)*factorial(b+d)) / (factorial(a)*factorial(b)*factorial(c)*factorial(d)*factorial(a+b+c+d))
  return(p)
}
```

Теперь вычислим доверительный уровень вероятности:

```{r alpha}
alpha <- P(tab) + P(tab2) + P(tab3) + P(tab4)
alpha
```

Видно, что p-уровень значимости больше, чем 0.05, а значит мы принимаем нулевую гипотезу.

Заметим, что, используя функцию fisher.test() в R, можно получить аналогичный результат:

```{r fisher}
fisher.test(tab, alternative="less")
```

Видно, что p-уровень значимости больше, чем 0.05, а значит мы принимаем нулевую гипотезу.

## 3. Ассимптотический критерий хи-квадрат Пирсона

В общем случае, когда работаем с таблицей размерности $s \times r$, статистика $\chi^2$ имеет вид 
$$\chi^2 = n(\sum_{i=1}^r\sum_{j=1}^s \frac{n_{ij}^2}{n_{i*}n_{*j}} - 1).$$

В нашем случае таблица сопряженности имеет размерность $2 \times 2$.

```{r tab1.1}
tab
```

Поэтому рассмотрим частный случай: $\chi^2 = \frac{n(ad-bc)^2}{(a+b)(c+d)(a+c)(b+d)}$.

```{r chisq}
n <- 21
chisq <- (n*(tab[1,1]*tab[2,2] - tab[1,2]*tab[2,1])^2) / ((tab[1,1]+tab[1,2])*(tab[2,1]+tab[2,2])*(tab[1,1]+tab[2,1])*(tab[1,2]+tab[2,2]))
chisq
```

Теперь найдем $p-value:$ 

```{r pl}
1 - pchisq(chisq, 1)
```
p-значение велико, значит нет основания отклонить гипотезу.

Если воспользоваться встроенной в R функцией chisq.test(), то получим:

```{r chi}
chisq.test(tab)
```

В данном случае видно, что аппроксимация может быть неверной, поэтому стоит обратиться к уже полученному результату точного критерия Фишера.

